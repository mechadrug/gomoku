<!DOCTYPE html>
<html lang="zh">
<head>
	<meta http-equiv="X-UA-Compatible" content="IE=11,10,9,edge">
	<meta name="keywords" content="Gaoling School of Artificial Intelligence">
	<meta name="description" content="Gaoling School of Artificial Intelligence">
 	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,Chrome=1">
	<title>Gaoling School of Artificial Intelligence</title>
	<link rel="stylesheet" href="../../css/swiper_hha.css?v=1719558955109">
	<link rel="stylesheet" href="../../css/animate.css?v=1719558955109">
	<link rel="stylesheet" href="../../css/bootstrap.min.css?v=1719558955109">
<link rel="stylesheet" href="../../css/layout.css?v=1719558955109">
<link rel="stylesheet" href="../../css/menucss.css?v=1719558955109">
	<script src="../../js/jquery.min.js?v=1719558955109"></script>
	<script src="../../js/wow.min.js?v=1719558955109"></script>
	<script src="../../js/swiper-4.1.6.min.js?v=1719558955109"></script>
<script src="../../js/bootstrap.min.js?v=1719558955109"></script>
<script src="../../js/jQuery.autoIMG.js?v=1719558955109"></script>
<script src="https://cdn.bootcss.com/jquery-mousewheel/3.1.13/jquery.mousewheel.min.js?v=1719558955109"></script>
<script type="text/javascript"> 
	var browser={
	    versions:function(){
	        var u = navigator.userAgent, app = navigator.appVersion;
	        return {//移动终端浏览器版本信息
	            trident: u.indexOf('Trident') > -1, //IE内核
	            presto: u.indexOf('Presto') > -1, //opera内核
	            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
	            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
	            mobile: !!u.match(/AppleWebKit.*Mobile.*/)||u.indexOf('iPad') > -1, //是否为移动终端
	            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
	            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
	            iPhone: u.indexOf('iPhone') > -1, //是否为iPhone或者QQHD浏览器
	            iPad: u.indexOf('iPad') > -1, //是否iPad
	            webApp: u.indexOf('Safari') == -1 //是否web应该程序，没有头部与底部
	        };
	    }(),
	}
	if(browser.versions.android || browser.versions.iPhone){
var oMeta = document.createElement('meta');
                oMeta.name = 'viewport';
                oMeta.content = 'width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no';
                document.getElementsByTagName('head')[0].appendChild(oMeta);
	}
</script>

</head>
<body>

<!--========== HEADER ==========-->
        <header class="header">
            <!-- Navbar -->
            <nav class="navbar" role="navigation">
                <div class="container">
                    <!-- Brand and toggle get grouped for better mobile display -->
                    <div class="menu-container">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="toggle-icon"></span>
                        </button>

                        <!-- Logo -->
                        <div class="navbar-logo">
                            <a class="navbar-logo-wrap" href="http://ai.ruc.edu.cn/english/index.htm">
                                <img class="navbar-logo-img" src="../../images/gsai_en_logo_white.png" >
                            </a>
                        </div>
                        <!-- End Logo -->
                    </div>

                    <!-- Collect the nav links, forms, and other content for toggling -->
                    <div class="collapse navbar-collapse nav-collapse">
                        <div class="menu-container">
                            <ul class="navbar-nav navbar-nav-right">
                                <!-- Home -->
                                <li class="nav-item">
                                    <a class="nav-item-child " href="../index.htm">
                                        Home
                                    </a>
                                </li>
                                <!-- End Home -->

                                <!-- About -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="../gsaiabout/introduction/index.htm">
                                        ABOUT US
                                    </a>
                                </li>
                                <!-- End About -->

                                <!-- Work -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="../GSAI_FACULTY/index.htm" >
                                        FACULTY & RESEARCH
                                    </a>
                                </li>
                                <!-- End Work -->

 <!-- Work -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="../academic/index.htm" >
                                        ACADEMIC
                                    </a>
                                </li>
                                <!-- End Work -->


                             <!-- Contact -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="index.htm">
                                        NEWS
                                    </a>
                                </li>
                                <!-- End Contact -->


								 <!-- Contact -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="../gsaijob/index.htm">
                                        JOB OPENING
                                    </a>
                                </li>
                                <!-- End Contact -->


								 <!-- Contact -->
                                <li class="nav-item">
                                    <a class="nav-item-child" style="color:#f4ca06" href="http://ai.ruc.edu.cn">
                                        CN(中文)
                                    </a>
                                </li>
                                <!-- End Contact -->
                            </ul>
                        </div>
                    </div>
                    <!-- End Navbar Collapse -->
                </div>
            </nav>
            <!-- Navbar -->
        </header>
        <!--========== END HEADER ==========--><!-- Features -->
<div class="body_bg" style="height: 200px">
    <div class="overlayer"></div>
    <div class="content-md container " style="height: 200px">
        <div class="row">
            <div class="col-sm-12 col-xs-12">
                <div class="text-white text-center"><h2 class="text-white">NEWS</h2></div>


            </div>
        </div>

    </div>
</div>
<!-- End Features -->

<!-- Work -->
<div class="pagecontent">
    <div class="container">
        <div class="row">
            <div class="col-xs-3 col-sm-3 hidden-xs">
               
<nav>
                        <div id="menu" class="white menu">
                            <div class="menu-header">MENU </div>
                            <ul>
                                <li  ><a  href="../index.htm">HOME</a></li>
                                <li><a href="#">ABOUT US</a>
                                    <ul class="submenu">
                                        <li ><a href="../gsaiabout/introduction/index.htm"> INTRODUCTION</a></li>
                                        <li ><a href="../gsaiabout/administrators/index.htm"> CURRENT ADMINISTRATORS</a></li>
                                    </ul>
                                </li>
                                <li ><a href="../GSAI_FACULTY/index.htm">FACULTY & RESEARCH</a></li>
                                <li ><a href="../academic/index.htm">ACADEMIC</a></li>
                                <li  class="active" ><a href="index.htm">NEWS</a></li>
                                <li ><a href="../gsaijob/index.htm">JOB OPENING</a></li>
                            </ul>
                          
                        </div>
                    </nav>

<!--
<div class="leftmenu">
</div>-->            </div>
            <div class="col-xs-12 col-sm-9">
                <div class="articlecontent">
                     <div style="font-size:14px;"><i class="fa fa-bank"></i> 
 				<a href="../index.htm">GSAIHOME<span style="margin-left:10px;margin-right:10px">/</span></a>
				<a href="index.htm">NEWS<span style="margin-left:10px;margin-right:10px">/</span></a>
				</div>
                <div class="articletitle"><h2>Chinese University Creates Sora-like Video Diffusion Transformer, Paper Accepted by ICLR 2024</h2><small><span></span></small>

<div style="text-align:center;margin-top:10px"><span>Date：2024-03-29</span>  <span style="margin-left:20px">Visits：<span id="visit_count"  style="padding:0;"><script src="/wm/api/visit/get/article?siteID=b789c5741b814b2998e36b9a58c98f96&articleID=4d4ab8f1d77e46e9ab52ec9922231d0f" async>
</script>
</span></div></div>
                 <div class="articlecontent" id="desc">
                   <p style="line-height:180%;text-indent:2em;font-size:14pt"><br></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">The release of OpenAI’s Sora on February 16 marked a huge breakthrough in the domain of video generation. Leveraging Diffusion Transformer architecture, Sora distinguishes itself from the majority of mainstream methods (extended from 2D Stable Diffusion) on the market.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">The reasons behind Sora’s insistence on using diffusion transformers can be found in a paper published in ICLR 2024 (VDT: General-purpose Video Diffusion Transformers via Mask Modeling) around the same time.</p> 
<p style="text-align:center;text-indent:0px"><img src="../../images/2024-06/fc2e09cd432f4567a6d78da29c8531f5.png" width="553" height="190"></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">This endeavor, led by a research team from Renmin University of China (RUC) in collaboration with the University of California, Berkeley, the University of Hong Kong, and others, was first unveiled to the public on the arXiv website in May 2023. The team proposed a transformer-based unified framework for video generation - Video Diffusion Transformer (VDT), and provided detailed explanations for the adoption of transformers.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">l Paper Title：VDT: General-purpose Video Diffusion Transformers via Mask Modeling</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">l Website：Openreview: https://openreview.net/pdf?id='''Un0rgm9f04</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">l arXiv address: https://arxiv.org/abs/2305.13311</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">l Project Address：VDT: General-purpose Video Diffusion Transformers via Mask Modeling</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">l Code Address：<a href="https://github.com/RERV/VDT" style="text-decoration:none" rel="nofollow">https://github.com/RERV/VDT</a></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">1. Advantages and Innovations of VDT</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Researchers say that VDT has the following advantages in the domain of video generation:</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Unlike U-Net, which is primarily designed for images, transformers harness their robust tokenization and attention mechanisms to capture long-range or irregular temporal dependencies. Therefore, they are better at processing temporal data.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">For video diffusion to align with the real world, the model must learn (or memorize) world [n1] knowledge, such as spatiotemporal relationships and physical laws. Thus, model capacity is a crucial component of video diffusion. Transformers have proven to be highly scalable, as evidenced by models like PaLM with up to 540 billion parameters, while the largest 2D U-Net model at the time was only 2.6 billion parameters (SDXL). This makes transformers more suitable for tackling the challenges of video generation compared to 3D U-Net.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">The field of video generation encompasses tasks such as unconditional generation, video prediction, interpolation, and text-to-image generation. Previous research largely focused on individual tasks, often requiring the incorporation of specialized modules for downstream fine-tuning. Additionally, these tasks involve diverse conditioning information, which may vary across different frames and modalities, necessitating a powerful architecture capable of handling varying input lengths and modalities. The introduction of transformers enables the unification of these tasks.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">The innovations of VDT primarily include the following aspects:</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">The application of transformers to diffusion-based video generation demonstrates the immense potential of transformers in the field of video generation. VDT excels in its ability to capture temporal dependencies, enabling the generation of temporally consistent video frames, including simulating the physics and dynamics of 3D objects over time.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">The proposal of a unified spatiotemporal masking mechanism enables VDT to handle various video generation tasks, achieving widespread application of the technology. VDT’s flexible handling of conditioning information, with simple token spatial concatenation, effectively unifies information of different lengths and modalities. Moreover, through integration with the proposed spatiotemporal masking mechanism, VDT becomes a general-purpose video diffuser for harnessing a range of video generation tasks, including unconditional generation, subsequent frame prediction, interpolation, image-to-video generation, and video frame completion, without modifying the model structure.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">2. Detailed Analysis of VDT’s Structure</p> 
<p style="text-align:center;text-indent:0px"><img src="../../images/2024-06/9802f524fb934c4fa7d90c1bd96320ec.png" width="480" height="233"></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">The VDT framework bears striking resemblances to that of Sora, encompassing the following components:</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Input/Output Features: The objective of VDT is to generate a video clip ∈ RF ×H×W×3, consisting of F frames of size H × W. However, using raw pixels as input for VDT can lead to extremely heavy computation, particularly when F is large. To address this issue, VDT takes inspiration from LDM and projects the video into a latent space using a pre-trained VAE tokenizer from LDM. This speeds up VDT by reducing the input and output to latent feature/noise F ∈ RF ×H/8×W/8×C, consisting of F frame latent features of size H/8 × W/8. Here, 8 is the downsample rate of the VAE tokenizer, and C denotes the latent feature dimension.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Linear Embedding: Following the approach of Vision Transformers, VDT divides the latent video feature representation into non-overlapping patches of size N×N.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Spatio-Temporal Transformer Block: Inspired by the success of space-time self-attention in video modeling, VDT inserts a temporal attention layer into the transformer block to obtain the temporal modeling ability. Specifically, each transformer block comprises a multi-head temporal-attention layer, a multi-head spatial-attention layer, and a fully connected feedforward network.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Comparing Sora’s latest technical report, it can be observed that VDT and Sora exhibit only subtle differences in implementation details.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">First, VDT adopts a method where attention mechanisms are separately applied in the temporal and spatial dimensions, whereas Sora merges the temporal and spatial dimensions to handle them through a single attention mechanism. This separated attention approach is quite common in the video domain and is often viewed as a compromise under memory constraints. The choice of separated attention in VDT is also driven by considerations of limited computational resources. Sora’s robust video dynamics capabilities may stem from its holistic spatio-temporal attention mechanism.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Secondly, unlike VDT, Sora also considers the integration of text conditions. Previous research has explored text condition integration based on Transformers (e.g., PIXART-α). Here, it is speculated that Sora may further incorporate cross-attention mechanisms into its modules. Of course, directly concatenating text and noise frames as conditional inputs is also a potential approach.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">During the research process of VDT, the research team replaced the commonly used backbone network U-Net with transformers. This not only validates the effectiveness of transformers in video diffusion tasks, demonstrating advantages in scalability and continuity enhancement but also stimulates further consideration of the potential value Transformation.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">With the success of GPT and the popularity of autoregressive (AR) models, researchers have begun to explore deeper applications of transformers in the field of video generation, pondering whether they can provide new methods for achieving visual intelligence. One closely related task in the video generation domain is video prediction. The idea of predicting the next video frame as a path to visual intelligence may seem simple, but it is actually a problem that many researchers are collectively concerned with.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Given this, researchers hope to further adapt and optimize their models for video prediction tasks. Video prediction can also be viewed as conditional generation, where the given conditional frames are the preceding frames of the video. VDT primarily considers the following three conditional generation methods:</p> 
<p style="text-align:center;text-indent:0px"><img src="../../images/2024-06/c5af2a7731814f5f8ce8f710e37db553.png" width="509" height="273"></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Adaptive Layer Normalization: A direct approach to achieve video prediction is to integrate conditional frame features into the layer normalization of the VDT Block, similar to how we integrate time information into the diffusion process.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Cross-Attention: Researchers also explore the use of cross-attention as a video prediction scheme, where conditional frames serve as keys and values, while the noisy frame serves as the query. This allows for the fusion of conditional information with the noisy frame. Prior to entering the cross-attention layer, the features of conditional frames are extracted using a VAE tokenizer and are patchfied. Additionally, spatial and temporal position embeddings are added to help VDT learn corresponding information within the conditional frames.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Token Concatenation: Since the VDT model adopts pure transformer architectures, using conditional frames directly as input tokens is a more intuitive approach. Researchers achieve this by concatenating conditional frames (latent features) and noisy frames at the token level, which are then fed into VDT. Subsequently, they split the output frame sequence of VDT and used the predicted frames for the diffusion process, as shown in Figure 3 (b). Researchers find that this approach demonstrates the fastest convergence speed and provides better performance in the final results compared to the first two methods. Moreover, researchers find that even when using fixed-length conditional frames during training, VDT can still accept conditional frames of arbitrary lengths as input and output consistent predicted features.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Under the framework of VDT, to achieve video prediction tasks, no modifications need to be made to the network structure; only changes to the model’s inputs are required. This discovery raises an intuitive question: can we further leverage this scalability to extend VDT to more diverse video generation tasks, such as generating videos from images, without introducing any additional modules or parameters?</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">By reviewing VDT’s functionalities in unconditional generation and video prediction, the only difference lies in the types of input features. Specifically, inputs can either be pure noise latent features or concatenations of conditions and noise latent features. Subsequently, researchers introduce Unified Spatial-Temporal Mask Modeling to unify conditional inputs, as shown in Figure 4.</p> 
<p style="text-align:center;text-indent:0px"><img src="../../images/2024-06/d834852125d84c7382d969d701396715.png" width="464" height="174"></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">3. Performance Evaluation of VDT</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Through the aforementioned methods, the VDT model not only seamlessly handles unconditional video generation and video prediction tasks but also extends to a broader range of video generation domains, such as video frame interpolation, by simply adjusting input features. This demonstration of flexibility and scalability showcases the powerful potential of the VDT framework, providing new directions and possibilities for future video generation technologies.</p> 
<p style="text-align:center;text-indent:0px"><img src="../../images/2024-06/c6bd60a32f3c4e3d9f03fe50ac1b2fd1.png" width="561" height="683"></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Interestingly, in addition to text-to-video, OpenAI also demonstrated Sora’s impressive performance in other tasks, including image-based generation, video prediction, and fusion of different video clips, which are very similar to downstream tasks supported by the Unified Spatial-Temporal Mask Modeling proposed by researchers. Additionally, references to Kaiming’s MAE are also cited, suggesting that Sora likely employs a training method similar to MAE at the lower levels.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Researchers also explored the simulation of simple physical laws by the generative model VDT. They conducted experiments on the Physion dataset, where VDT used the first 8 frames as conditional frames and predicted the next 8 frames. In the first example (top two rows) and the third example (bottom two rows), VDT successfully simulated physical processes, including a ball moving along a parabolic trajectory and a ball rolling on flat ground and colliding with a cylinder. In the second example (middle two rows), VDT captured the velocity/momentum of the ball as it stopped before colliding with the cylinder. This demonstrates that the Transformer architecture is capable of learning certain physical laws.</p> 
<p style="text-align:center;text-indent:0px"><img src="../../images/2024-06/613aab4f877a4a27a6bef0c4253bffad.png" width="504" height="376"></p> 
<p style="text-align:center;text-indent:0px"><img src="../../images/2024-06/b723fb93755a46e491cd69ab278d0387.png" width="529" height="350"></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Researchers also conducted some ablation studies on the VDT model’s structure. The results indicate that reducing the patch size, increasing the number of layers, and increasing the Hidden Size can further improve the model’s performance. The position of temporal and spatial attention and the number of attention heads have minimal impact on the model’s results. With the same GFlops, some design trade-offs are needed, and overall, there is no significant difference in the model’s performance. However, increasing GFlops leads to better results, demonstrating the scalability of VDT or Transformer architecture.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">The effectiveness and flexibility of VDT demonstrate the efficacy of transformer structures in video generation. Due to limitations in computational resources, VDT was only experimented on a few small-scale academic datasets. We look forward to future research exploring new directions and applications in video generation technology based on VDT and hope that Chinese companies will soon launch domestic Sora models.</p> 
                </div>
                </div>

            </div>
        </div>
    </div>
</div>

<!--========== FOOTER ==========-->
        <footer class="footer">
            <!-- Links -->
                <div class="content container">
                    <div class="row">
                        <div class="col-sm-4 col-xs-12 sm-margin-b-30 hidden-xs">
                            <img src="../../images/gsai_en_logo_white.png" style="height:60px">
                        </div>

                        <div class="col-sm-5 col-xs-12">
                            <!-- List -->
                            <ul class="list-unstyled footer-list">
                                <li class="footer-list-item">Email: <a href="#">gsai@ruc.edu.cn</a></li>
                                <li class="footer-list-item">Mobile: <a href="#">86-10-62511257</a></li>
                            </ul>
                            <!-- End List -->
                        </div>
                        <div class="col-sm-3 col-xs-12">
                            <!-- List -->
                            <ul class="list-unstyled footer-list">
                                    <li class="footer-list-item">Copyright©2020 Gaoling School of <br>Artificial Intelligence</li>
                            </ul>
                            <!-- End List -->
                        </div>
                    </div>
                    <!--// end row -->
                </div>
  
            <!-- End Links -->
        </footer>
        <!--========== END FOOTER ==========-->

        <!-- Back To Top -->
        <a href="javascript:void(0);" class="js-back-to-top back-to-top">Top</a>
<script src="../../js/menuscript.js?v=1719558955109"></script>
<script>
$(function(){
		$("#desc").autoIMG();
	});
</script>

<script>	
	window.onload=function(){  
		
	}			
	</script>
<script type="text/javascript">
	jQuery.ajax({
		url: '/wm/api/visit/write/article',
		type: 'get',
		data: {
			siteID: 'b789c5741b814b2998e36b9a58c98f96',
			articleID:'4d4ab8f1d77e46e9ab52ec9922231d0f',
			articleName:encodeURIComponent("Chinese University Creates Sora-like Video Diffusion Transformer, Paper Accepted by ICLR 2024")
		}
	})
</script>

    </body>
</html><script type='text/javascript' src='../../g_style/g_article.js?v=1719558955109'></script>
</body></html>