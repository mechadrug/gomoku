<!DOCTYPE html>
<html lang="zh">
<head>
	<meta http-equiv="X-UA-Compatible" content="IE=11,10,9,edge">
	<meta name="keywords" content="Gaoling School of Artificial Intelligence">
	<meta name="description" content="Gaoling School of Artificial Intelligence">
 	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,Chrome=1">
	<title>Gaoling School of Artificial Intelligence</title>
	<link rel="stylesheet" href="../../css/swiper_hha.css">
	<link rel="stylesheet" href="../../css/animate.css">
	<link rel="stylesheet" href="../../css/bootstrap.min.css">
<link rel="stylesheet" href="../../css/layout.css">
<link rel="stylesheet" href="../../css/menucss.css">
	<script src="../../js/jquery.min.js"></script>
	<script src="../../js/wow.min.js"></script>
	<script src="../../js/swiper-4.1.6.min.js"></script>
<script src="../../js/bootstrap.min.js"></script>
<script src="../../js/jQuery.autoIMG.js"></script>
<script src="https://cdn.bootcss.com/jquery-mousewheel/3.1.13/jquery.mousewheel.min.js"></script>
<script type="text/javascript"> 
	var browser={
	    versions:function(){
	        var u = navigator.userAgent, app = navigator.appVersion;
	        return {//移动终端浏览器版本信息
	            trident: u.indexOf('Trident') > -1, //IE内核
	            presto: u.indexOf('Presto') > -1, //opera内核
	            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
	            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
	            mobile: !!u.match(/AppleWebKit.*Mobile.*/)||u.indexOf('iPad') > -1, //是否为移动终端
	            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
	            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
	            iPhone: u.indexOf('iPhone') > -1, //是否为iPhone或者QQHD浏览器
	            iPad: u.indexOf('iPad') > -1, //是否iPad
	            webApp: u.indexOf('Safari') == -1 //是否web应该程序，没有头部与底部
	        };
	    }(),
	}
	if(browser.versions.android || browser.versions.iPhone){
var oMeta = document.createElement('meta');
                oMeta.name = 'viewport';
                oMeta.content = 'width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no';
                document.getElementsByTagName('head')[0].appendChild(oMeta);
	}
</script>

</head>
<body>

<!--========== HEADER ==========-->
        <header class="header">
            <!-- Navbar -->
            <nav class="navbar" role="navigation">
                <div class="container">
                    <!-- Brand and toggle get grouped for better mobile display -->
                    <div class="menu-container">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="toggle-icon"></span>
                        </button>

                        <!-- Logo -->
                        <div class="navbar-logo">
                            <a class="navbar-logo-wrap" href="http://ai.ruc.edu.cn/english/index.htm">
                                <img class="navbar-logo-img" src="../../images/gsai_en_logo_white.png" >
                            </a>
                        </div>
                        <!-- End Logo -->
                    </div>

                    <!-- Collect the nav links, forms, and other content for toggling -->
                    <div class="collapse navbar-collapse nav-collapse">
                        <div class="menu-container">
                            <ul class="navbar-nav navbar-nav-right">
                                <!-- Home -->
                                <li class="nav-item">
                                    <a class="nav-item-child " href="../index.htm">
                                        Home
                                    </a>
                                </li>
                                <!-- End Home -->

                                <!-- About -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="../gsaiabout/introduction/index.htm">
                                        ABOUT US
                                    </a>
                                </li>
                                <!-- End About -->

                                <!-- Work -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="../GSAI_FACULTY/index.htm" >
                                        FACULTY & RESEARCH
                                    </a>
                                </li>
                                <!-- End Work -->

 <!-- Work -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="../academic/index.htm" >
                                        ACADEMIC
                                    </a>
                                </li>
                                <!-- End Work -->


                             <!-- Contact -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="index.htm">
                                        NEWS
                                    </a>
                                </li>
                                <!-- End Contact -->


								 <!-- Contact -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="../gsaijob/index.htm">
                                        JOB OPENING
                                    </a>
                                </li>
                                <!-- End Contact -->


								 <!-- Contact -->
                                <li class="nav-item">
                                    <a class="nav-item-child" style="color:#f4ca06" href="http://ai.ruc.edu.cn">
                                        CN(中文)
                                    </a>
                                </li>
                                <!-- End Contact -->
                            </ul>
                        </div>
                    </div>
                    <!-- End Navbar Collapse -->
                </div>
            </nav>
            <!-- Navbar -->
        </header>
        <!--========== END HEADER ==========--><!-- Features -->
<div class="body_bg" style="height: 200px">
    <div class="overlayer"></div>
    <div class="content-md container " style="height: 200px">
        <div class="row">
            <div class="col-sm-12 col-xs-12">
                <div class="text-white text-center"><h2 class="text-white">NEWS</h2></div>


            </div>
        </div>

    </div>
</div>
<!-- End Features -->

<!-- Work -->
<div class="pagecontent">
    <div class="container">
        <div class="row">
            <div class="col-xs-3 col-sm-3 hidden-xs">
               
<nav>
                        <div id="menu" class="white menu">
                            <div class="menu-header">MENU </div>
                            <ul>
                                <li  ><a  href="../index.htm">HOME</a></li>
                                <li><a href="#">ABOUT US</a>
                                    <ul class="submenu">
                                        <li ><a href="../gsaiabout/introduction/index.htm"> INTRODUCTION</a></li>
                                        <li ><a href="../gsaiabout/administrators/index.htm"> CURRENT ADMINISTRATORS</a></li>
                                    </ul>
                                </li>
                                <li ><a href="../GSAI_FACULTY/index.htm">FACULTY & RESEARCH</a></li>
                                <li ><a href="../academic/index.htm">ACADEMIC</a></li>
                                <li  class="active" ><a href="index.htm">NEWS</a></li>
                                <li ><a href="../gsaijob/index.htm">JOB OPENING</a></li>
                            </ul>
                          
                        </div>
                    </nav>

<!--
<div class="leftmenu">
</div>-->            </div>
            <div class="col-xs-12 col-sm-9">
                <div class="articlecontent">
                     <div style="font-size:14px;"><i class="fa fa-bank"></i> 
 				<a href="../index.htm">GSAIHOME<span style="margin-left:10px;margin-right:10px">/</span></a>
				<a href="index.htm">NEWS<span style="margin-left:10px;margin-right:10px">/</span></a>
				</div>
                <div class="articletitle"><h2>Five papers from GSAI Accepted by CCF A-category Conference CVPR</h2><small><span></span></small>

<div style="text-align:center;margin-top:10px"><span>Date：2022-03-18</span>  <span style="margin-left:20px">Visits：<span id="visit_count"  style="padding:0;"><script src="/wm/api/visit/get/article?siteID=b789c5741b814b2998e36b9a58c98f96&articleID=ad42bb69cf9b4fd2b973eb88dcb27429" async>
</script>
</span></div></div>
                 <div class="articlecontent" id="desc">
                   <p style="line-height:180%;text-indent:2em;font-size:14pt"><br></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">On March 2th, the results of the acceptance of papers of the international academic conference CVPR 2022 were announced, and 5 papers from teachers and students in Gaoling School of Artificial Intelligence, Renmin University of China were accepted. The International Conference on Computer Vision and Pattern Recognition (CVPR) is a top-level conference in the field of computer vision and pattern recognition organized by IEEE, which is held annually around the world. The year 2022 is the 40th conference, and it will be held June 19-24 in New Orleans, Louisiana in a mixed online and offline format.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Paper Introduction</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Title: COTS: Collaborative Two-Stream Vision-Language Pre-Training Model</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>for Cross-Modal Retrieval</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Authors: Haoyu Lu, Nanyi Fei, Yuqi Huo, Yizhao Gao, Zhiwu Lu, Ji-Rong Wen</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Corresponding Author: Zhiwu Lu</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Paper Overview: Large-scale single-stream pre-training has shown dramatic performance in image-text retrieval. Regrettably, it faces low inference efficiency due to heavy attention layers. Recently, two-stream methods like CLIP and ALIGN with high inference efficiency have also shown promising performance, however, they only consider instance-level alignment between the two streams (thus there is still room for improvement). To overcome these limitations, we propose a novel Collaborative Two-Stream vision-language pre-training model termed COTS for image-text retrieval by enhancing cross-modal interaction. In addition to instance-level alignment via momentum contrastive learning, we leverage two extra levels of cross-modal interactions in our COTS: (1) Token-level interaction -- a masked vision-language modeling (MVLM) learning objective is devised without using a cross-stream network module, where variational autoencoder is imposed on the visual encoder to generate visual tokens for each image. (2) Task-level interaction -- a KL-alignment learning objective is devised between text-to-image and image-to-text retrieval tasks, where the probability distribution per task is computed with the negative queues in momentum contrastive learning. Under a fair comparison setting, our COTS achieves the highest performance among all two-stream methods and comparable performance (but with 10,800x faster in inference) w.r.t. the latest single-stream methods. Importantly, our COTS is also applicable to text-to-video retrieval, yielding new state-of-the-art on the widely-used MSR-VTT dataset.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Paper Introduction</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Title：Balanced Audio-visual Learning via On-the-fly Gradient Modulation</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Authors：Xiaokang Peng*, Yake Wei*, Andong Deng, Dong Wang, Di Hu</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Corresponding Author：Di Hu</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Paper Overview:Multimodal learning helps to comprehensively understand the world, by integrating different senses. Accordingly, multiple input modalities are expected to boost model performance, but we actually find that they are not fully exploited even when the multimodal model outperforms its uni-modal counterpart. Specifically, in this paper we point out that existing multimodal discriminative models, in which uniform objective is designed for all modalities, could remain under-optimized uni-modal representations, caused by another dominated modality in some scenarios, e.g., sound in blowing wind event, vision in drawing picture event, etc. To alleviate this optimization imbalance, we propose on-the-fly gradient modulation to adaptively control the optimization of each modality, via monitoring the discrepancy of their contribution towards the learning objective. Further, an extra Gaussian noise that changes dynamically is introduced to avoid possible generalization drop caused by gradient modulation. As a result, we achieve considerable improvement over common fusion methods on different multimodal tasks, and this simple strategy can also boost existing multimodal methods, which illustrates its efficacy and versatility.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Paper Introduction</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Title：Learning to Answer Questions in Dynamic Audio-Visual Scenarios</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Authors：Guangyao Li*, Yake Wei*, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, Di Hu*</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Corresponding Author：Di Hu</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Paper Overview：In this paper, we focus on the Audio-Visual Question Answering (AVQA) task, which aims to answer questions regarding different visual objects, sounds, and their associations in videos. The problem requires comprehensive multimodal understanding and spatio-temporal reasoning over audio-visual scenes. To benchmark this task and facilitate our study, we introduce a large-scale AVQA dataset, which contains more than 45K question-answer pairs covering 33 different question templates spanning over different modalities and question types. We develop several baselines and introduce a spatio-temporal grounded audio-visual network for the AVQA problem. Our results demonstrate that AVQA benefits from multisensory perception and our model outperforms recent A-, V-, and AVQA approaches. We believe that our built dataset has the potential to serve as testbed for evaluating and promoting progress in audio-visual scene understanding and spatio-temporal reasoning. Code and dataset are available at <a href="http://ayameyao.github.io/st-avqa/" style="text-decoration:none" rel="nofollow">http://ayameyao.github.io/st-avqa/</a>.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Paper Introduction</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Title: Deep Safe Multi-view Clustering: Reducing the Risk of Clustering Performance Degradation Caused by View Increase</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Authors: Huayi Tang, Yong Liu</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Corresponding Author: Yong Liu</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Paper Overview: Multi-view clustering has been shown to boost clustering performance by effectively mining the complementary information from multiple views. However, we observe that sometimes learning from data with more views is not guaranteed to achieve better clustering performance than from data with fewer views. To address this issue, we propose a general deep learning based framework to reduce the risk of clustering performance degradation caused by view increase. Concretely, the model is required to extract complementary information and discard the meaningless noise by automatically selecting features. These two learning procedures are integrated into a unified framework by the proposed optimization objective. In theory, the empirical clustering risk of the proposed framework is no higher than learning from data before the view increase and data of the new increased single view. Also, the expected clustering risk of the framework under divergence-based loss is no higher than that with high probability. Comprehensive experiments on benchmark datasets demonstrate the effectiveness and superiority of the proposed framework in achieving safe multi-view clustering.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Paper Introduction</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Title: Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Authors: Hongwei Xue*, Tiankai Hang*, Yanhong Zeng*, Yuchong Sun*, Bei Liu, Huan Yang, Jianlong Fu, Baining Guo</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Paper Overview: We study joint video and language (VL) pre-training to enable cross-modality learning and benefit plentiful downstream VL tasks. Existing works either extract low-quality video features or learn limited text embedding, while neglecting that high-resolution videos and diversified semantics can significantly improve cross-modality learning. In this paper, we propose a novel High-resolution and Diversified VIdeo-LAnguage pre-training model (HD-VILA) for many visual tasks. In particular, we collect a large dataset with two distinct properties: 1) the first high-resolution dataset including 371.5k hours of 720p videos, and 2) the most diversified dataset covering 15 popular YouTube categories. To enable VL pre-training, we jointly optimize the HD-VILA model by a hybrid Transformer that learns rich spatiotemporal features, and a multimodal Transformer that enforces interactions of the learned video features with diversified texts. Our pre-training model achieves new state-of-the-art results in 10 VL understanding tasks and 2 more novel text-to-visual generation tasks. For example, we outperform SOTA models with relative increases of 38.5% R@1 in zero-shot MSR-VTT text-to-video retrieval task, and 53.6% in high-resolution dataset LSMDC. The learned VL embedding is also effective in generating visually pleasing and semantically relevant results in text-to-visual manipulation and super-resolution tasks.</p> 
                </div>
                </div>

            </div>
        </div>
    </div>
</div>

<!--========== FOOTER ==========-->
        <footer class="footer">
            <!-- Links -->
                <div class="content container">
                    <div class="row">
                        <div class="col-sm-4 col-xs-12 sm-margin-b-30 hidden-xs">
                            <img src="../../images/gsai_en_logo_white.png" style="height:60px">
                        </div>

                        <div class="col-sm-5 col-xs-12">
                            <!-- List -->
                            <ul class="list-unstyled footer-list">
                                <li class="footer-list-item">Email: <a href="#">gsai@ruc.edu.cn</a></li>
                                <li class="footer-list-item">Mobile: <a href="#">86-10-62511257</a></li>
                            </ul>
                            <!-- End List -->
                        </div>
                        <div class="col-sm-3 col-xs-12">
                            <!-- List -->
                            <ul class="list-unstyled footer-list">
                                    <li class="footer-list-item">Copyright©2020 Gaoling School of <br>Artificial Intelligence</li>
                            </ul>
                            <!-- End List -->
                        </div>
                    </div>
                    <!--// end row -->
                </div>
  
            <!-- End Links -->
        </footer>
        <!--========== END FOOTER ==========-->

        <!-- Back To Top -->
        <a href="javascript:void(0);" class="js-back-to-top back-to-top">Top</a>
<script src="../../js/menuscript.js"></script>
<script>
$(function(){
		$("#desc").autoIMG();
	});
</script>

<script>	
	window.onload=function(){  
		
	}			
	</script>
<script type="text/javascript">
	jQuery.ajax({
		url: '/wm/api/visit/write/article',
		type: 'get',
		data: {
			siteID: 'b789c5741b814b2998e36b9a58c98f96',
			articleID:'ad42bb69cf9b4fd2b973eb88dcb27429',
			articleName:encodeURIComponent("Five papers from GSAI Accepted by CCF A-category Conference CVPR")
		}
	})
</script>

    </body>
</html><script type='text/javascript' src='../../g_style/g_article.js'></script>
</body></html>