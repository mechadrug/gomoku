<!DOCTYPE html>
<html lang="zh">
<head>
	<meta http-equiv="X-UA-Compatible" content="IE=11,10,9,edge">
	<meta name="keywords" content="Gaoling School of Artificial Intelligence">
	<meta name="description" content="Gaoling School of Artificial Intelligence">
 	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,Chrome=1">
	<title>Gaoling School of Artificial Intelligence</title>
	<link rel="stylesheet" href="../../css/swiper_hha.css">
	<link rel="stylesheet" href="../../css/animate.css">
	<link rel="stylesheet" href="../../css/bootstrap.min.css">
<link rel="stylesheet" href="../../css/layout.css">
<link rel="stylesheet" href="../../css/menucss.css">
	<script src="../../js/jquery.min.js"></script>
	<script src="../../js/wow.min.js"></script>
	<script src="../../js/swiper-4.1.6.min.js"></script>
<script src="../../js/bootstrap.min.js"></script>
<script src="../../js/jQuery.autoIMG.js"></script>
<script src="https://cdn.bootcss.com/jquery-mousewheel/3.1.13/jquery.mousewheel.min.js"></script>
<script type="text/javascript"> 
	var browser={
	    versions:function(){
	        var u = navigator.userAgent, app = navigator.appVersion;
	        return {//移动终端浏览器版本信息
	            trident: u.indexOf('Trident') > -1, //IE内核
	            presto: u.indexOf('Presto') > -1, //opera内核
	            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
	            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
	            mobile: !!u.match(/AppleWebKit.*Mobile.*/)||u.indexOf('iPad') > -1, //是否为移动终端
	            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
	            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
	            iPhone: u.indexOf('iPhone') > -1, //是否为iPhone或者QQHD浏览器
	            iPad: u.indexOf('iPad') > -1, //是否iPad
	            webApp: u.indexOf('Safari') == -1 //是否web应该程序，没有头部与底部
	        };
	    }(),
	}
	if(browser.versions.android || browser.versions.iPhone){
var oMeta = document.createElement('meta');
                oMeta.name = 'viewport';
                oMeta.content = 'width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no';
                document.getElementsByTagName('head')[0].appendChild(oMeta);
	}
</script>

</head>
<body>

<!--========== HEADER ==========-->
        <header class="header">
            <!-- Navbar -->
            <nav class="navbar" role="navigation">
                <div class="container">
                    <!-- Brand and toggle get grouped for better mobile display -->
                    <div class="menu-container">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="toggle-icon"></span>
                        </button>

                        <!-- Logo -->
                        <div class="navbar-logo">
                            <a class="navbar-logo-wrap" href="http://ai.ruc.edu.cn/english/index.htm">
                                <img class="navbar-logo-img" src="../../images/gsai_en_logo_white.png" >
                            </a>
                        </div>
                        <!-- End Logo -->
                    </div>

                    <!-- Collect the nav links, forms, and other content for toggling -->
                    <div class="collapse navbar-collapse nav-collapse">
                        <div class="menu-container">
                            <ul class="navbar-nav navbar-nav-right">
                                <!-- Home -->
                                <li class="nav-item">
                                    <a class="nav-item-child " href="../index.htm">
                                        Home
                                    </a>
                                </li>
                                <!-- End Home -->

                                <!-- About -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="../gsaiabout/introduction/index.htm">
                                        ABOUT US
                                    </a>
                                </li>
                                <!-- End About -->

                                <!-- Work -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="../GSAI_FACULTY/index.htm" >
                                        FACULTY & RESEARCH
                                    </a>
                                </li>
                                <!-- End Work -->

 <!-- Work -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="../academic/index.htm" >
                                        ACADEMIC
                                    </a>
                                </li>
                                <!-- End Work -->


                             <!-- Contact -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="index.htm">
                                        NEWS
                                    </a>
                                </li>
                                <!-- End Contact -->


								 <!-- Contact -->
                                <li class="nav-item">
                                    <a class="nav-item-child" href="../gsaijob/index.htm">
                                        JOB OPENING
                                    </a>
                                </li>
                                <!-- End Contact -->


								 <!-- Contact -->
                                <li class="nav-item">
                                    <a class="nav-item-child" style="color:#f4ca06" href="http://ai.ruc.edu.cn">
                                        CN(中文)
                                    </a>
                                </li>
                                <!-- End Contact -->
                            </ul>
                        </div>
                    </div>
                    <!-- End Navbar Collapse -->
                </div>
            </nav>
            <!-- Navbar -->
        </header>
        <!--========== END HEADER ==========--><!-- Features -->
<div class="body_bg" style="height: 200px">
    <div class="overlayer"></div>
    <div class="content-md container " style="height: 200px">
        <div class="row">
            <div class="col-sm-12 col-xs-12">
                <div class="text-white text-center"><h2 class="text-white">NEWS</h2></div>


            </div>
        </div>

    </div>
</div>
<!-- End Features -->

<!-- Work -->
<div class="pagecontent">
    <div class="container">
        <div class="row">
            <div class="col-xs-3 col-sm-3 hidden-xs">
               
<nav>
                        <div id="menu" class="white menu">
                            <div class="menu-header">MENU </div>
                            <ul>
                                <li  ><a  href="../index.htm">HOME</a></li>
                                <li><a href="#">ABOUT US</a>
                                    <ul class="submenu">
                                        <li ><a href="../gsaiabout/introduction/index.htm"> INTRODUCTION</a></li>
                                        <li ><a href="../gsaiabout/administrators/index.htm"> CURRENT ADMINISTRATORS</a></li>
                                    </ul>
                                </li>
                                <li ><a href="../GSAI_FACULTY/index.htm">FACULTY & RESEARCH</a></li>
                                <li ><a href="../academic/index.htm">ACADEMIC</a></li>
                                <li  class="active" ><a href="index.htm">NEWS</a></li>
                                <li ><a href="../gsaijob/index.htm">JOB OPENING</a></li>
                            </ul>
                          
                        </div>
                    </nav>

<!--
<div class="leftmenu">
</div>-->            </div>
            <div class="col-xs-12 col-sm-9">
                <div class="articlecontent">
                     <div style="font-size:14px;"><i class="fa fa-bank"></i> 
 				<a href="../index.htm">GSAIHOME<span style="margin-left:10px;margin-right:10px">/</span></a>
				<a href="index.htm">NEWS<span style="margin-left:10px;margin-right:10px">/</span></a>
				</div>
                <div class="articletitle"><h2>Papers of teachers and students from Gaoling Artificial Intelligence School of Renmin University were accepted by CCF Class A conference CVPR 2023</h2><small><span></span></small>

<div style="text-align:center;margin-top:10px"><span>Date：2023-03-15</span>  <span style="margin-left:20px">Visits：<span id="visit_count"  style="padding:0;"><script src="/wm/api/visit/get/article?siteID=b789c5741b814b2998e36b9a58c98f96&articleID=cd79bcd2e85f4413a925c20f1cab6eff" async>
</script>
</span></div></div>
                 <div class="articlecontent" id="desc">
                   <h4>CVPR-2023</h4> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">On February 27th, the acceptance results of CVPR 2023, the Class A international academic conference recommended by the China Computer Society (CCF), were announced. And 6 papers from teachers and students of the Gaoling School of Artificial Intelligence, Renmin University of China were accepted. The International Conference on Computer Vision and Pattern Recognition (CVPR) is a top-level conference in the field of computer vision and pattern recognition organized by IEEE , which is held annually around the world. The acceptance rate of CVPR 2023 is 25.78%.</p> 
<p style="text-align:center;text-indent:0px"><img src="../../images/2023-03/f89e9eadbb6e4beb892ff3d8034daee2.png" width="607" height="217"></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Introduction 1</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Title：</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">HOTNAS：Hierarchical Optimal Transport for Neural Architecture Search</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Authors：</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Jiechao Yang，Yong Liu，Hongteng Xu</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Corresponding Author：Yong Liu</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Overview：</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Instead of searching the entire network directly, current NAS approaches are increasingly searching for multiple relatively small cells to reduce search costs. A major challenge is to jointly measure the similarity of cell micro-architectures and the difference in macro-architectures between different cell-based networks. Recently, optimal transport (OT) has been successfully applied to NAS as it can capture both the operation and structure similarity of different networks. Unfortunately, existing OT for NAS methods either ignore the similarity between cells or focus on searching for a single cell architecture. To solve these problems, we propose a hierarchical optimal transport metric called HOTNN for measuring the similarity of different networks. The cell-level similarity computes OT distance between cells in different networks by considering the similarity of each node in different cells and the differences in the information flow costs between node pairs within each cell in terms of the operation and structure information. The network-level similarity calculates OT distance between networks by considering both the cell-level similarity and the difference in the global position of each cell in their respective networks. We then explore HOTNN in a Bayesian optimization framework called HOTNAS to show its effectiveness across different tasks. Extensive experiments demonstrate that HOTNAS can find a network architecture with better performance in multiple modular cell-based search spaces.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Introduction 2</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Title：</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Fair Scratch Tickets: Finding Fair Sparse Networks without Weight Training</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Authors：</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Pengwei Tang*，Wei Yao*, Zhicong Li, Yong Liu</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Corresponding Author：Yong Liu</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Overview：</strong><span style="font-size:14pt;text-indent:2em">Recent studies suggest that computer vision models come at the risk of compromising fairness. There are extensive works to alleviate unfairness in computer vision using pre-processing, in-processing, and post-processing methods. In this paper, we lead a novel fairness-aware learning paradigm for in-processing methods through the lens of the lottery ticket hypothesis (LTH) in the context of computer vision fairness. We randomly initialize a dense neural network and find appropriate binary masks for the weights to obtain a fair sparse subnetworks without any weight training. Interestingly, to the best of our knowledge, we are the first to discover that such sparse subnetworks with inborn fairness exist in randomly initialized networks, achieving an accuracy-fairness trade-off comparable to that of dense neural networks trained with existing fairness-aware in-processing approaches. We term these fair subnetworks as Fair Scratch Tickets (FSTs). We also theoretically provide fairness and accuracy guarantees for them. In our experiments, we investigate the existence of FSTs on various datasets, target attributes, random initialization methods, sparsity patterns, and fairness surrogates. We also find that FSTs can transfer across datasets and investigate other properties of FSTs.</span></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Introduction 3</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Title：</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Modeling Video as Stochastic Processes for Fine-Grained Video Representation Learning</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Authors：</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Heng Zhang，Daqing Liu，Qi Zheng，Bing Su</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Corresponding Author：Bing Su</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Overview：</strong>A meaningful video is semantically coherent and changes smoothly. However, most existing fine-grained video representation learning methods learn frame-wise features by aligning frames across videos or exploring relevance between multiple views, neglecting the inherent dynamic process of each video. In this paper, we propose to learn video representations by modeling Video as Stochastic Processes (VSP) via a novel process-based contrastive learning framework, which aims to discriminate between video processes and simultaneously capture the temporal dynamics in the processes. Specifically, we enforce the embeddings of the frame sequence of interest to approximate a goal-oriented stochastic process, i.e., Brownian bridge, in the latent space via a process-based contrastive loss. To construct the Brownian bridge, we adapt specialized sampling strategies under different annotations for both self-supervised and weakly-supervised learning. Experimental results on four datasets show that VSP stands as a state-of-the-art method for various video understanding tasks, including phase progression, phase classification, and frame retrieval.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Introduction 4</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Title：</strong>Transfer Knowledge from Head to Tail: Uncertainty Calibration under Long-tailed Distribution</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Authors</strong>：Jiahao Chen, Bing Su</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Corresponding Author：Bing Su</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Overview：</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">How to estimate the uncertainty of a given model is a crucial problem. Current calibration techniques treat different classes equally and thus implicitly assume that the distribution of training data is balanced, but ignore the fact that real-world data often follows a long-tailed distribution. In this paper, we explore the problem of calibrating the model trained from a long-tailed distribution. Due to the difference between the imbalanced training distribution and balanced test distribution, existing calibration methods such as temperature scaling can not generalize well to this problem. Specific calibration methods for domain adaptation are also not applicable because they rely on unlabeled target domain instances which are not available. Models trained from a long-tailed distribution tend to be more overconfident to head classes. To this end, we propose a novel knowledge-transferring-based calibration method by estimating the importance weights for samples of tail classes to realize long-tailed calibration. Our method models the distribution of each class as a Gaussian distribution and views the source statistics of head classes as a prior to calibrate the target distributions of tail classes. We adaptively transfer knowledge from head classes to get the target probability density of tail classes. The importance weight is estimated by the ratio of the target probability density over the source probability density. Extensive experiments on CIFAR-10-LT, MNIST-LT, CIFAR-100-LT, and ImageNet-LT datasets demonstrate the effectiveness of our method.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Introduction 5</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Title：</strong>All are Worth Words: A ViT Backbone for Diffusion Models</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Authors</strong>：Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Hang Su, Chongxuan Li, Jun Zhu</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Corresponding Author：</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Chongxuan Li, Jun Zhu</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Overview:</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Vision transformers (ViT) have shown promise in various vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion models. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion models. U-ViT is characterized by treating all inputs including the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and classconditional image generation, as well as text-to-image generation tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on ImageNet 256×256, and 5.48 in text-to-image generation on MS-COCO, among methods without accessing large external datasets during the training of generative models.</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Introduction 6</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Title:</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Compacting Binary Neural Networks by Sparse Kernel Selection</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Authors:</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Yikai Wang, Wenbing Huang, Yinpeng Dong, Fuchun Sun, Anbang Yao</p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt"><strong>Paper Overview:</strong></p> 
<p style="line-height:180%;text-indent:2em;font-size:14pt">Binary Neural Network (BNN) represents convolution weights with 1-bit values, which enhances the efficiency of both storage and computation. This paper is motivated by a previously revealed phenomenon that the binary kernels in successful BNNs are nearly power-law distributed: their values are mostly clustered into a small number of codewords. This phenomenon encourages us to compact typical BNNs and obtain further close performance through learning non-repetitive kernels within a subspace of the whole possible values. Specifically, we regard the binarization process as kernel grouping in terms of a binary codebook, and our task lies in learning to select a smaller subset of codewords from the full codebook. We then leverage the Gumbel-Sinkhorn technique to approximate the codeword selection process, and develop the Permutation Straight-Through Estimator (PSTE) that is able to not only optimize the selection process end-to-end but also maintain the non-repetitive occupancy of the selected codewords. Experiments on image classification and object detection provably verify that our method is able to reduce both the model size and bit-wise computational costs, with only a slight accuracy drop compared with state-of-the-art BNNs.</p> 
                </div>
                </div>

            </div>
        </div>
    </div>
</div>

<!--========== FOOTER ==========-->
        <footer class="footer">
            <!-- Links -->
                <div class="content container">
                    <div class="row">
                        <div class="col-sm-4 col-xs-12 sm-margin-b-30 hidden-xs">
                            <img src="../../images/gsai_en_logo_white.png" style="height:60px">
                        </div>

                        <div class="col-sm-5 col-xs-12">
                            <!-- List -->
                            <ul class="list-unstyled footer-list">
                                <li class="footer-list-item">Email: <a href="#">gsai@ruc.edu.cn</a></li>
                                <li class="footer-list-item">Mobile: <a href="#">86-10-62511257</a></li>
                            </ul>
                            <!-- End List -->
                        </div>
                        <div class="col-sm-3 col-xs-12">
                            <!-- List -->
                            <ul class="list-unstyled footer-list">
                                    <li class="footer-list-item">Copyright©2020 Gaoling School of <br>Artificial Intelligence</li>
                            </ul>
                            <!-- End List -->
                        </div>
                    </div>
                    <!--// end row -->
                </div>
  
            <!-- End Links -->
        </footer>
        <!--========== END FOOTER ==========-->

        <!-- Back To Top -->
        <a href="javascript:void(0);" class="js-back-to-top back-to-top">Top</a>
<script src="../../js/menuscript.js"></script>
<script>
$(function(){
		$("#desc").autoIMG();
	});
</script>

<script>	
	window.onload=function(){  
		
	}			
	</script>
<script type="text/javascript">
	jQuery.ajax({
		url: '/wm/api/visit/write/article',
		type: 'get',
		data: {
			siteID: 'b789c5741b814b2998e36b9a58c98f96',
			articleID:'cd79bcd2e85f4413a925c20f1cab6eff',
			articleName:encodeURIComponent("Papers of teachers and students from Gaoling Artificial Intelligence School of Renmin University were accepted by CCF Class A conference CVPR 2023")
		}
	})
</script>

    </body>
</html><script type='text/javascript' src='../../g_style/g_article.js'></script>
</body></html>